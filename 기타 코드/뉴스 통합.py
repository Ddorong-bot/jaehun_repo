import pandas as pd
import feedparser
import os
import re
import json
import time
import hashlib
import requests
import unicodedata
import google.generativeai as genai
from datetime import datetime, timedelta, timezone
from email.utils import parsedate_to_datetime
from bs4 import BeautifulSoup
from newspaper import Article, Config

# ==========================================
# [ì‚¬ìš©ì ì„¤ì •] í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •í•˜ì„¸ìš”
# ==========================================
# 1. íŒŒì¼ ê²½ë¡œ
INPUT_EXCEL_FILENAME = "C:/Users/Choi/Desktop/ì¼ë³¸ rss.xlsx"  # ì–¸ë¡ ì‚¬, RSSì£¼ì†Œ ì»¬ëŸ¼ í•„ìš”
OUTPUT_JSON_FILENAME = "C:/Users/Choi/Desktop/ì¼ë³¸_ë‰´ìŠ¤_ìµœì¢…ê²°ê³¼.json"

# 2. Gemini API í‚¤
GEMINI_API_KEY = ""  # ì—¬ê¸°ì— API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”

# 3. ìˆ˜ì§‘ ì„¤ì •
DAYS_LIMIT = 1  # ë©°ì¹  ì „ ë‰´ìŠ¤ê¹Œì§€ ìˆ˜ì§‘í• ì§€
SOURCE_TYPE = "RSS"  # ì„œë²„ì— ë³´ë‚¼ sourceType (ëŒ€ë¬¸ì ê¶Œì¥)

# 4. í•„í„°ë§ í‚¤ì›Œë“œ (í•´ì™¸/êµ­ì œ ë‰´ìŠ¤ ì œì™¸)
EXCLUDE_KEYWORDS = ['world', 'global', 'international', 'overseas', 'foreign', 'êµ­ì œ', 'í•´ì™¸', 'english']

# 5. ë³¸ë¬¸ ì œê±° ë¬¸êµ¬
GARBAGE_PHRASES = [
    "We use cookies", "cookie policy", "Accept all", "Manage preferences",
    "This website uses cookies", "All rights reserved", "ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤",
    "ë¬´ë‹¨ ì „ì¬ ë° ì¬ë°°í¬ ê¸ˆì§€", "ê¸°ì êµ¬ë…"
]
# ==========================================

# Gemini ì„¤ì •
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)
    model = genai.GenerativeModel('models/gemini-2.0-flash')
else:
    print("âš ï¸ ê²½ê³ : GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¶„ë¥˜ ê¸°ëŠ¥ì´ ì‘ë™í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    model = None

def clean_html(raw_html):
    """HTML íƒœê·¸ ì œê±°"""
    if not raw_html: return ""
    cleanr = re.compile('<.*?>')
    return re.sub(cleanr, '', raw_html).strip()

def normalize_text(text: str) -> str:
    """í…ìŠ¤íŠ¸ ì •ê·œí™” (í•´ì‹œìš©)"""
    if not text: return ""
    text = str(text)
    text = unicodedata.normalize("NFKC", text)
    text = text.strip()
    text = re.sub(r"\s+", " ", text)
    return text

def compute_content_hash(title: str, content: str) -> str:
    """SHA-256 í•´ì‹œ ìƒì„±"""
    payload = f"{normalize_text(title)}\n{normalize_text(content)}"
    return hashlib.sha256(payload.encode("utf-8")).hexdigest()

def normalize_date(date_str):
    """
    ë‚ ì§œë¥¼ ISO 8601 í˜•ì‹(YYYY-MM-DDTHH:mm:ss+HH:MM)ìœ¼ë¡œ í†µì¼
    ì„œë²„ 400 ì—ëŸ¬ ë°©ì§€ìš© í•µì‹¬ í•¨ìˆ˜
    """
    if not date_str:
        return datetime.now().astimezone().isoformat()
    
    try:
        # 1. ì´ë¯¸ ISO í˜•ì‹ì¸ ê²½ìš°
        datetime.fromisoformat(date_str.replace('Z', '+00:00'))
        return date_str
    except ValueError:
        pass

    try:
        # 2. RSS/ì´ë©”ì¼ í˜•ì‹ (Fri, 19 Dec 2025...)
        dt = parsedate_to_datetime(date_str)
        return dt.isoformat()
    except Exception:
        pass
    
    # 3. íŒŒì‹± ì‹¤íŒ¨ ì‹œ í˜„ì¬ ì‹œê°„ ë°˜í™˜ (ë°ì´í„° ëˆ„ë½ ë°©ì§€)
    return datetime.now().astimezone().isoformat()

def classify_text_with_ai(title, content):
    """Geminië¥¼ ì´ìš©í•œ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
    if not model: return "others"
    
    summary_text = f"Title: {title}\nContent: {str(content)[:500]}"
    prompt = f"""
    Analyze the news article and classify into exactly one: [Politics, Economy, Tech, Others].
    Output ONLY the category name.
    Article: {summary_text}
    """
    
    for _ in range(2): # ì¬ì‹œë„ ë¡œì§
        try:
            response = model.generate_content(prompt)
            cat = response.text.strip().replace("'", "").replace('"', "").lower()
            if 'politics' in cat: return 'politics'
            if 'economy' in cat: return 'economy'
            if 'tech' in cat: return 'tech'
            return 'others'
        except:
            time.sleep(1)
    return "others"

def is_foreign_news(entry):
    """í•´ì™¸ ë‰´ìŠ¤ í•„í„°ë§"""
    link = entry.get('link', '').lower()
    title = entry.get('title', '').lower()
    for kw in EXCLUDE_KEYWORDS:
        if f'/{kw}/' in link or f'/{kw}.' in link or kw in title:
            return True
    if 'tags' in entry:
        for tag in entry.tags:
            if any(kw in tag.get('term', '').lower() for kw in EXCLUDE_KEYWORDS):
                return True
    return False

def get_full_article(url):
    """Newspaper3k + BeautifulSoup í•˜ì´ë¸Œë¦¬ë“œ ìˆ˜ì§‘"""
    headers = {'User-Agent': 'Mozilla/5.0 (compatible; Googlebot/2.1)'}
    try:
        # 1. Newspaper3k
        config = Config()
        config.browser_user_agent = headers['User-Agent']
        config.request_timeout = 10
        config.fetch_images = False
        
        article = Article(url, config=config)
        article.download()
        article.parse()
        content = article.text.strip()

        # 2. ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ BS4ë¡œ ê°•ì œ ìˆ˜ì§‘
        if len(content) < 200:
            res = requests.get(url, headers=headers, timeout=10)
            soup = BeautifulSoup(res.content, 'html.parser')
            paras = [p.get_text().strip() for p in soup.find_all('p') if len(p.get_text().strip()) > 30]
            forced_content = '\n\n'.join(paras)
            if len(forced_content) > len(content):
                content = forced_content

        # 3. ì •ì œ
        lines = [line for line in content.split('\n') if not any(g in line for g in GARBAGE_PHRASES)]
        return '\n'.join(lines)
    except:
        return ""

def main():
    if not os.path.exists(INPUT_EXCEL_FILENAME):
        print(f"âŒ ì˜¤ë¥˜: ì…ë ¥ íŒŒì¼('{INPUT_EXCEL_FILENAME}')ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(">>> ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ì²˜ë¦¬ ì‹œì‘...")
    df_urls = pd.read_excel(INPUT_EXCEL_FILENAME)
    
    cutoff_date = datetime.now() - timedelta(days=DAYS_LIMIT)
    articles_list = []
    
    processed_count = 0
    
    for _, row in df_urls.iterrows():
        press_name = row.get('ì–¸ë¡ ì‚¬', 'Unknown')
        rss_url = row.get('RSSì£¼ì†Œ', '')
        
        if not rss_url or pd.isna(rss_url): continue
        
        print(f"\nğŸ“° [{press_name}] ì²˜ë¦¬ ì¤‘...")
        try:
            feed = feedparser.parse(rss_url)
        except:
            print(f"   - ì ‘ì† ì‹¤íŒ¨: {rss_url}")
            continue

        for entry in feed.entries:
            # í•„í„°ë§
            if is_foreign_news(entry): continue
            
            # ë‚ ì§œ í™•ì¸
            date_parsed = entry.get('published_parsed', entry.get('updated_parsed'))
            if date_parsed and datetime(*date_parsed[:6]) < cutoff_date:
                continue

            link = entry.get('link', '')
            title = entry.get('title', '')
            raw_date = entry.get('published', entry.get('updated', ''))
            
            # ë³¸ë¬¸ ìˆ˜ì§‘
            content = get_full_article(link)
            if not content:
                content = "[ìš”ì•½] " + clean_html(entry.get('summary', ''))
            
            if len(content) < 50: continue # ë„ˆë¬´ ì§§ìœ¼ë©´ ê±´ë„ˆëœ€

            print(f"   Checking: {title[:20]}...")

            # ë°ì´í„° ê°€ê³µ (í•´ì‹œ, ë‚ ì§œ, ë¶„ë¥˜)
            c_hash = compute_content_hash(title, content)
            fmt_date = normalize_date(raw_date)
            category = classify_text_with_ai(title, content) # ì†Œë¬¸ìë¡œ ë°˜í™˜ë¨
            
            # externalId ìƒì„± (ê³ ìœ ì„± ë³´ì¥ ë…¸ë ¥)
            ext_id = f"{press_name}-{int(time.time())}-{processed_count}"

            # ìµœì¢… JSON ê°ì²´ êµ¬ì¡° (ingest_sample.json ê¸°ì¤€)
            article_obj = {
                "sourceType": SOURCE_TYPE,  # RSS (ëŒ€ë¬¸ì)
                "contentHash": c_hash,
                "externalId": ext_id,       # ì„œë²„ì—ì„œ ìš”êµ¬í•  ìˆ˜ ìˆì–´ ì¶”ê°€
                "sourceName": press_name,
                "categoryCode": category,   # politics (ì†Œë¬¸ì)
                "url": link,
                "title": title,
                "content": content,
                "author": press_name,       # ì‘ì„±ì ì—†ìœ¼ë©´ ì–¸ë¡ ì‚¬ëª…
                "publishedAt": fmt_date,    # ISO 8601 í˜•ì‹
                "fetchedAt": datetime.now().astimezone().isoformat()
            }
            
            articles_list.append(article_obj)
            processed_count += 1
            time.sleep(0.5) # API ë° ì„œë²„ ë¶€í•˜ ì¡°ì ˆ

    # ìµœì¢… ì €ì¥
    final_data = {"articles": articles_list}
    
    with open(OUTPUT_JSON_FILENAME, 'w', encoding='utf-8') as f:
        json.dump(final_data, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… [ì™„ë£Œ] ì´ {len(articles_list)}ê±´ ì €ì¥ë¨.")
    print(f"ğŸ“ íŒŒì¼ ìœ„ì¹˜: {OUTPUT_JSON_FILENAME}")

if __name__ == "__main__":
    main()