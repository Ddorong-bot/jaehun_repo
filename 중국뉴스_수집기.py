# -*- coding: utf-8 -*-
"""ì¤‘êµ­ë‰´ìŠ¤ ìˆ˜ì§‘ê¸°_251222

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AeZjmDzXH27BEO73L4y3PMRhu8h1viA_
"""

# ============================================
# ì¼ì¼ ì¤‘êµ­ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œìŠ¤í…œ (í”„ë¡œë•ì…˜)
#
# ê¸°ëŠ¥:
# - í•œêµ­ ì‹œê°„ëŒ€ ê¸°ì¤€ ë‹¹ì¼ ì‘ì„± ë‰´ìŠ¤ë§Œ ìˆ˜ì§‘
# - ëª…í™•í•œ ì‘ì„±ì¼ì´ ì—†ìœ¼ë©´ ì œì™¸
# - ë…„ì›”ì¼(YYYYMMDD) íƒ€ì„ìŠ¤íƒ¬í”„ë¡œ íŒŒì¼ ìƒì„±
# - ë™ì¼ ë‚ ì§œ ì¬ì‹¤í–‰ ì‹œ ë®ì–´ì“°ê¸°
#
# Google Colabìš©
# ============================================

!pip install anthropic
from anthropic import Anthropic
from google.colab import userdata
import json
import re
import requests
from bs4 import BeautifulSoup
import time
from datetime import datetime
import pytz
from urllib.parse import urljoin
import os

# ============================================
# ì„¤ì •
# ============================================

OUTPUT_DIR = "/content/collect-output"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# í•œêµ­ ì‹œê°„ëŒ€ (UTC+9)
korea_tz = pytz.timezone('Asia/Seoul')

# API ì„¤ì •
try:
    API_KEY = userdata.get('CLAUDE_API_KEY')
    if not API_KEY:
        raise ValueError("API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    print("âœ… API í‚¤ ë¡œë“œ ì™„ë£Œ\n")
except Exception as e:
    print(f"âŒ ì˜¤ë¥˜: {e}")
    raise

client = Anthropic(api_key=API_KEY)

# ë‰´ìŠ¤ ì†ŒìŠ¤
NEWS_SOURCES = {
    "äººæ°‘æ—¥æŠ¥": {
        "url": "http://www.people.com.cn/",
        "category": "ì •ì¹˜/ì •ì±…",
        "reliability": "A"
    },
    "æ–°åç½‘": {
        "url": "http://www.xinhuanet.com/",
        "category": "ì¢…í•©ë‰´ìŠ¤",
        "reliability": "A"
    },
    "å¤®è§†æ–°é—»": {
        "url": "http://news.cctv.com/",
        "category": "ì¢…í•©ë‰´ìŠ¤",
        "reliability": "A"
    },
    "ç»æµè§‚å¯Ÿç½‘": {
        "url": "http://www.eeo.com.cn/",
        "category": "ê²½ì œ",
        "reliability": "B"
    },
    "è´¢ç»ç½‘": {
        "url": "http://www.caijing.com.cn/",
        "category": "ê¸ˆìœµ/ê²½ì œ",
        "reliability": "B"
    },
    "æ¾æ¹ƒæ–°é—»": {
        "url": "http://www.thepaper.cn/",
        "category": "ì¡°ì‚¬ë³´ë„",
        "reliability": "B"
    },
    "36æ°ª": {
        "url": "http://www.36kr.com/",
        "category": "ê¸°ìˆ ",
        "reliability": "C"
    }
}

COUNTRY = "ä¸­å›½"

AI_PROMPT = """
ì¤‘êµ­ ë‰´ìŠ¤ ë¶„ì„ ê¸°ìì…ë‹ˆë‹¤.
ê° ì œëª©ì´ ì‹¤ì œ ë‰´ìŠ¤ì¸ì§€ íŒë‹¨í•˜ì„¸ìš”.

ë‰´ìŠ¤ ì•„ë‹˜: "æ‰«ç ", "403", "Forbidden", ì™¸êµ­ì–´ë§Œ, ì—ëŸ¬ë©”ì‹œì§€
ë‰´ìŠ¤: ê·¸ ì™¸ ëª¨ë‘

JSONìœ¼ë¡œë§Œ ì‘ë‹µ. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ì—†ìŒ.
{"result": [{"idx": 1, "is_news": true, "importance": 7, "category": "ê²½ì œ"}]}
"""

# ============================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ============================================

def get_today_date():
    """í•œêµ­ ì‹œê°„ëŒ€ ê¸°ì¤€ ì˜¤ëŠ˜ ë‚ ì§œ (YYYY-MM-DD)"""
    now = datetime.now(korea_tz)
    return now.strftime("%Y-%m-%d")

def get_db_filename():
    """DB íŒŒì¼ëª… ìƒì„± (ë…„ì›”ì¼ íƒ€ì„ìŠ¤íƒí”„)"""
    now = datetime.now(korea_tz)
    date_str = now.strftime("%Y%m%d")
    return os.path.join(OUTPUT_DIR, f"chinanews_collection_{date_str}.json")

def check_duplicate(new_record, existing_records):
    """ì¤‘ë³µ í™•ì¸ (ì–¸ë¡ ì‚¬ + ì œëª© + ê¸°ì‚¬ë‚ ì§œ)"""
    for existing in existing_records:
        if (new_record['news_source'] == existing['news_source'] and
            new_record['title'] == existing['title'] and
            new_record['article_date'] == existing['article_date']):
            return True
    return False

# ============================================
# DB ê´€ë¦¬
# ============================================

def load_or_create_db():
    """DB ë¡œë“œ ë˜ëŠ” ì‹ ê·œ ìƒì„±"""
    db_file = get_db_filename()

    if os.path.exists(db_file):
        try:
            with open(db_file, 'r', encoding='utf-8') as f:
                db = json.load(f)
                print(f"âœ… ê¸°ì¡´ DB ë¡œë“œ: {len(db.get('records', []))}ê°œ ê¸°ì‚¬\n")
                return db
        except Exception as e:
            print(f"âš ï¸ DB ë¡œë“œ ì‹¤íŒ¨, ìƒˆë¡œ ìƒì„±\n")

    db = {
        "metadata": {
            "country": COUNTRY,
            "sources": list(NEWS_SOURCES.keys()),
            "total_records": 0,
            "last_updated": None,
            "categories": ["ì •ì¹˜", "ê²½ì œ", "ê¸°ìˆ ", "ê¸°íƒ€"]
        },
        "records": []
    }
    print(f"âœ… ìƒˆ DB ìƒì„±\n")
    return db

def save_db(db):
    """DB ì €ì¥"""
    try:
        db_file = get_db_filename()

        with open(db_file, 'w', encoding='utf-8') as f:
            json.dump(db, f, ensure_ascii=False, indent=2)

        file_size = os.path.getsize(db_file) / 1024
        filename = os.path.basename(db_file)

        print(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ")
        print(f"   ğŸ“„ íŒŒì¼: {filename}")
        print(f"   ğŸ“Š í¬ê¸°: {file_size:.1f} KB")
        print(f"   ğŸ“Œ ì´ ê¸°ì‚¬: {len(db['records'])}ê°œ\n")
        return True
    except Exception as e:
        print(f"âŒ ì €ì¥ ì‹¤íŒ¨: {str(e)}\n")
        return False

# ============================================
# ë‰´ìŠ¤ ìˆ˜ì§‘
# ============================================

def collect_news(source_name, source_info):
    """ë‰´ìŠ¤ ì œëª©ê³¼ ë§í¬ ìˆ˜ì§‘"""
    try:
        url = source_info["url"]
        print(f"  ğŸ”— {source_name} [{source_info['reliability']}]...", end=" ")

        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

        response = requests.get(url, headers=headers, timeout=10)
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.content, 'html.parser')

        spam_keywords = [
            'Â·', 'æ ç›®', 'åˆ†ç±»', 'å¯¼èˆª', 'èœå•', 'æ›´å¤š', 'é¦–é¡µ',
            'è®¢é˜…', 'ç™»å½•', 'æ³¨å†Œ', 'é¢‘é“', 'æœç´¢', 'ç”¨æˆ·', 'å¹¿å‘Š',
            'Ğ ÑƒÑÑĞºĞ¸Ğ¹', 'PortuguÃªs', 'English', 'FranÃ§ais', 'æ—¥æœ¬èª',
            '403', '404', 'Forbidden', 'Error', 'é”™è¯¯',
            'æ‰«ç ', 'å¾®ä¿¡', 'é•¿æŒ‰', 'å…³æ³¨',
            'å¤‡æ¡ˆ', 'ç‰ˆæƒ', 'ICP', 'è”ç³»', 'å…³äº',
            'ç²¾å½©å†…å®¹', 'æœ€æ–°è§†é¢‘', 'THE LATEST',
        ]

        raw_news = []

        # h1, h2, h3ì—ì„œ ìˆ˜ì§‘
        for tag_name in ['h1', 'h2', 'h3']:
            for tag in soup.find_all(tag_name):
                title = tag.get_text(strip=True)
                if any(keyword in title for keyword in spam_keywords) or not (8 < len(title) < 150):
                    continue

                link = ""
                a_tag = tag.find('a')
                if a_tag and a_tag.get('href'):
                    link = a_tag.get('href')
                    if not link.startswith('http'):
                        link = urljoin(url, link)

                raw_news.append({
                    "source": source_name,
                    "source_reliability": source_info["reliability"],
                    "source_category": source_info["category"],
                    "title_zh": title,
                    "link": link
                })

        # a íƒœê·¸ì—ì„œë„ ìˆ˜ì§‘
        for a_tag in soup.find_all('a'):
            title = a_tag.get_text(strip=True)
            link = a_tag.get('href', '')

            if any(keyword in title for keyword in spam_keywords) or not (8 < len(title) < 150):
                continue
            if not link or link.startswith('javascript') or any(item['title_zh'] == title for item in raw_news):
                continue

            if not link.startswith('http'):
                link = urljoin(url, link)

            raw_news.append({
                "source": source_name,
                "source_reliability": source_info["reliability"],
                "source_category": source_info["category"],
                "title_zh": title,
                "link": link
            })

        # ì¤‘ë³µ ì œê±°
        unique_news = []
        seen = set()
        for item in raw_news:
            if item['title_zh'] not in seen:
                seen.add(item['title_zh'])
                unique_news.append(item)

        # ëª¨ë“  ë‰´ìŠ¤ ìˆ˜ì§‘ (ì œí•œ ì—†ìŒ)
        print(f"âœ… {len(unique_news)}ê°œ")
        return unique_news

    except Exception as e:
        print(f"âŒ")
        return []

# ============================================
# ê¸°ì‚¬ ë‚ ì§œ ì¶”ì¶œ
# ============================================

def extract_article_date(link):
    """ì›ë¬¸ì—ì„œ ë°œí–‰ ë‚ ì§œ ì¶”ì¶œ (ëª…í™•í•œ ê²½ìš°ë§Œ)"""
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        response = requests.get(link, headers=headers, timeout=8)
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.content, 'html.parser')

        patterns = [
            r'(\d{4})[å¹´-](\d{1,2})[æœˆ-](\d{1,2})[æ—¥å·]',  # 2024å¹´12æœˆ21æ—¥
            r'(\d{4})-(\d{2})-(\d{2})',                     # 2024-12-21
        ]

        found_dates = []

        # ë©”íƒ€ íƒœê·¸ í™•ì¸
        for meta in soup.find_all('meta'):
            properties = [meta.get('property', '').lower(), meta.get('name', '').lower()]
            if any(p in ['publish_date', 'article:published_time', 'og:published_time']
                   for p in properties):
                content = meta.get('content', '')
                if content:
                    match = re.search(r'(\d{4})-(\d{2})-(\d{2})', content)
                    if match:
                        found_dates.append(('ë©”íƒ€', match.group(0)))

        # ë³¸ë¬¸ ì²˜ìŒ 2000ìì—ì„œ ë‚ ì§œ ì°¾ê¸°
        full_text = soup.get_text()
        for pattern in patterns:
            match = re.search(pattern, full_text[:2000])
            if match:
                if len(match.groups()) == 3:
                    year, month, day = match.groups()
                    year, month, day = int(year), int(month), int(day)

                    if 2020 <= year <= 2026 and 1 <= month <= 12 and 1 <= day <= 31:
                        date_str = f"{year:04d}-{month:02d}-{day:02d}"
                        found_dates.append(('ë³¸ë¬¸', date_str))

        # ì†ŒìŠ¤ ì •ë³´ì—ì„œ ë‚ ì§œ ì°¾ê¸°
        source_section = soup.find(class_=re.compile('(source|from|byline|info)', re.I))
        if source_section:
            source_text = source_section.get_text()
            for pattern in patterns:
                match = re.search(pattern, source_text)
                if match:
                    if len(match.groups()) == 3:
                        year, month, day = match.groups()
                        year, month, day = int(year), int(month), int(day)

                        if 2020 <= year <= 2026 and 1 <= month <= 12 and 1 <= day <= 31:
                            date_str = f"{year:04d}-{month:02d}-{day:02d}"
                            found_dates.append(('ì†ŒìŠ¤', date_str))

        # ê²°ê³¼ ê²€ì¦ (ëª¨ë“  ë‚ ì§œê°€ ì¼ì¹˜í•´ì•¼ í•¨)
        if not found_dates:
            return None

        unique_dates = set(date for _, date in found_dates)

        if len(unique_dates) == 1:
            return unique_dates.pop()
        else:
            # ë‚ ì§œ ë¶ˆì¼ì¹˜
            return None

    except Exception as e:
        return None

# ============================================
# ì›ë¬¸ ì¶”ì¶œ
# ============================================

def get_article_content(link):
    """ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ"""
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        response = requests.get(link, headers=headers, timeout=8)
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.content, 'html.parser')

        for tag in soup.find_all(['script', 'style', 'nav', 'footer', 'noscript', 'meta']):
            tag.decompose()

        content = ""

        article = soup.find('article')
        if article:
            text = article.get_text(strip=True)
            if len(text) > 50:
                content = text

        if not content or len(content) < 50:
            for selector in ['content', 'article', 'news', 'main']:
                div = soup.find(id=selector)
                if div:
                    text = div.get_text(strip=True)
                    if len(text) > 50:
                        content = text
                        break

        if not content or len(content) < 50:
            paragraphs = soup.find_all('p')
            if paragraphs:
                valid = [p.get_text(strip=True) for p in paragraphs
                        if len(p.get_text(strip=True)) > 20
                        and 'è´£ä»»ç¼–è¾‘' not in p.get_text()]
                if valid:
                    content = '\n'.join(valid[:15])

        if content:
            content = re.sub(r'\n\s*\n', '\n', content)
            if 'è´£ä»»ç¼–è¾‘' in content:
                content = content.split('è´£ä»»ç¼–è¾‘')[0]
            return content[:500].strip()

        return "ì›ë¬¸ ë¡œë“œ ì‹¤íŒ¨"
    except Exception as e:
        return "ì›ë¬¸ ë¡œë“œ ì‹¤íŒ¨"

# ============================================
# AI ê²€ì¦
# ============================================

def validate_news(raw_news_list, today_date):
    """AI ê²€ì¦ + ë‹¹ì¼ ë‚ ì§œ í•„í„°ë§ (ë°°ì¹˜ ì²˜ë¦¬)"""
    if not raw_news_list:
        return []

    print(f"\nğŸ“‹ ê²€ì¦í•  ë‰´ìŠ¤ ({len(raw_news_list)}ê°œ):")
    print(f"âš ï¸  ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ê²€ì¦ ì¤‘... (ë°°ì¹˜ë‹¹ 50ê°œ)\n")

    db_records = []
    batch_size = 50

    # ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
    for batch_idx in range(0, len(raw_news_list), batch_size):
        batch = raw_news_list[batch_idx:batch_idx + batch_size]
        batch_num = batch_idx // batch_size + 1
        total_batches = (len(raw_news_list) + batch_size - 1) // batch_size

        print(f"ğŸ” ë°°ì¹˜ {batch_num}/{total_batches} ê²€ì¦ ì¤‘... ({len(batch)}ê°œ)")

        news_text = "\n".join([
            f"{i}. [{item['source']}] {item['title_zh']}"
            for i, item in enumerate(batch, 1)
        ])

        try:
            response = client.messages.create(
                model="claude-opus-4-1-20250805",
                max_tokens=3000,
                system=AI_PROMPT,
                messages=[
                    {
                        "role": "user",
                        "content": f"ë‰´ìŠ¤ ê²€ì¦:\n{news_text}"
                    }
                ]
            )

            response_text = response.content[0].text.strip()

            # JSON íŒŒì‹±
            result = None
            try:
                result = json.loads(response_text)
            except json.JSONDecodeError:
                match = re.search(r'```json\s*(.*?)\s*```', response_text, re.DOTALL)
                if match:
                    try:
                        result = json.loads(match.group(1).strip())
                    except:
                        pass

                if result is None:
                    match = re.search(r'\{.*"result".*\}', response_text, re.DOTALL)
                    if match:
                        try:
                            result = json.loads(match.group(0))
                        except:
                            pass

            if result is None:
                print(f"âš ï¸  ë°°ì¹˜ {batch_num}: JSON íŒŒì‹± ì‹¤íŒ¨, ê±´ë„ˆëœ€\n")
                continue

            # ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼
            batch_added = 0
            for item in result.get("result", []):
                if item.get("is_news", False):
                    idx = item.get("idx", 1) - 1
                    if 0 <= idx < len(batch):
                        original = batch[idx]

                        # ë‚ ì§œ ì¶”ì¶œ
                        extracted_date = extract_article_date(original["link"]) if original["link"] else None

                        # í•„í„°ë§ 1: ëª…í™•í•œ ë‚ ì§œ
                        if extracted_date is None:
                            continue

                        # í•„í„°ë§ 2: ë‹¹ì¼ ë‰´ìŠ¤
                        if extracted_date != today_date:
                            continue

                        # ì›ë¬¸ ì¶”ì¶œ
                        content = get_article_content(original["link"]) if original["link"] else "ì›ë¬¸ ë¡œë“œ ì‹¤íŒ¨"
                        content_length = len(content)
                        content_status = "ì •ìƒ" if content_length >= 100 else ("ë¶ˆì™„ì „" if content_length >= 50 else "ì˜¤ë¥˜")

                        db_record = {
                            "collection_date": today_date,
                            "article_date": extracted_date,
                            "country": COUNTRY,
                            "news_source": original["source"],
                            "source_reliability": original["source_reliability"],
                            "source_category": original["source_category"],
                            "title": original["title_zh"],
                            "content": content,
                            "content_length": content_length,
                            "content_status": content_status,
                            "link": original["link"],
                            "category": item.get("category", "å…¶ä»–"),
                            "importance": item.get("importance", 5),
                            "is_validated": True
                        }

                        db_records.append(db_record)
                        batch_added += 1

            print(f"âœ… ë°°ì¹˜ {batch_num}: {batch_added}ê°œ ì¶”ê°€ë¨\n")
            time.sleep(1)  # API ë ˆì´íŠ¸ ì œí•œ

        except Exception as e:
            print(f"âš ï¸  ë°°ì¹˜ {batch_num} ì˜¤ë¥˜: {str(e)}\n")
            continue

    print(f"âœ… ê²€ì¦ ì™„ë£Œ: {len(db_records)}ê°œ (ë‹¹ì¼ ëª…í™•í•œ ì‘ì„±ì¼ë§Œ)\n")
    return db_records

# ============================================
# ë©”ì¸
# ============================================

def main():
    now = datetime.now(korea_tz)
    today_date = now.strftime("%Y-%m-%d")

    print("=" * 80)
    print("ğŸ“° ì¼ì¼ ì¤‘êµ­ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œìŠ¤í…œ")
    print("=" * 80)
    print(f"ğŸ“… ê¸°ì¤€ ë‚ ì§œ: {today_date} (í•œêµ­ ì‹œê°„)")
    print(f"â° ìˆ˜ì§‘ ì‹œê°„: {now.strftime('%H:%M:%S')}")
    print(f"ğŸ’¾ ì €ì¥ íŒŒì¼: chinanews_collection_{today_date.replace('-', '')}.json")
    print(f"ğŸ¯ ì¡°ê±´: ëª…í™•í•œ ì‘ì„±ì¼ + ì˜¤ëŠ˜ ì‘ì„±ëœ ë‰´ìŠ¤ë§Œ\n")

    # Step 0: DB ë¡œë“œ
    print("=" * 80)
    print("âœ“ Step 0: DB ë¡œë“œ")
    print("=" * 80)
    db = load_or_create_db()

    # Step 1: ë‰´ìŠ¤ ìˆ˜ì§‘
    print("=" * 80)
    print("âœ“ Step 1: ë‰´ìŠ¤ ìˆ˜ì§‘")
    print("=" * 80 + "\n")

    all_news = []
    for source_name, source_info in NEWS_SOURCES.items():
        news = collect_news(source_name, source_info)
        all_news.extend(news)
        time.sleep(1)

    print(f"\nâœ… ì´ {len(all_news)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘")

    if not all_news:
        print("âŒ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ì—†ìŒ")
        return

    # Step 2: ê²€ì¦
    print("\n" + "=" * 80)
    print("âœ“ Step 2: ê²€ì¦ (AI) + ë‚ ì§œ í•„í„°ë§")
    print("=" * 80)

    db_records = validate_news(all_news, today_date)

    if not db_records:
        print("âŒ ìœ íš¨í•œ ë‰´ìŠ¤ ì—†ìŒ (ëª…í™•í•œ ë‹¹ì¼ ì‘ì„± ë‰´ìŠ¤ ë¶€ì¬)")
        return

    # Step 3: ì¤‘ë³µ í•„í„°ë§
    print("=" * 80)
    print("ğŸ”„ ì¤‘ë³µ í•„í„°ë§")
    print("=" * 80)
    existing_records = db.get("records", [])
    print(f"ê¸°ì¡´: {len(existing_records)}ê°œ")
    print(f"ê²€ì¦ë¨: {len(db_records)}ê°œ\n")

    added_count = 0
    for record in db_records:
        if not check_duplicate(record, existing_records):
            db["records"].append(record)
            added_count += 1
            print(f"âœ… ì¶”ê°€: [{record['news_source']}] {record['title'][:50]}")
        else:
            print(f"â­ï¸  ì¤‘ë³µ: [{record['news_source']}] {record['title'][:50]}")

    db["metadata"]["total_records"] = len(db["records"])
    db["metadata"]["last_updated"] = today_date

    print(f"\nğŸ“Š ì¶”ê°€ ê²°ê³¼: {added_count}ê°œ ì‹ ê·œ\n")

    if added_count == 0:
        print("âš ï¸ ëª¨ë“  ë‰´ìŠ¤ê°€ ì¤‘ë³µ. DB ì—…ë°ì´íŠ¸í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return

    # Step 4: ì €ì¥
    print("=" * 80)
    print("âœ“ Step 3: DB ì €ì¥")
    print("=" * 80 + "\n")

    if save_db(db):
        print(f"ğŸ“ˆ ìˆ˜ì§‘ í†µê³„:")
        print(f"   ìˆ˜ì§‘: {len(all_news)}ê°œ")
        print(f"   ê²€ì¦: {len(db_records)}ê°œ (ë‹¹ì¼ ëª…í™•ì¼)\n   ì‹ ê·œ: {added_count}ê°œ")
        print(f"   ì¤‘ë³µ: {len(db_records) - added_count}ê°œ")
        print(f"\nğŸ“Š DB í˜„í™©:")
        print(f"   ì´ ê¸°ì‚¬: {len(db['records'])}ê°œ")
        print(f"   ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: {db['metadata']['last_updated']}")

if __name__ == "__main__":
    main()